{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9801,"sourceType":"datasetVersion","datasetId":6763}],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from datasets import load_dataset\nimport pandas as pd\nimport nltk\nimport torch\nfrom nltk.corpus import stopwords\nfrom sklearn.metrics import average_precision_score\nfrom transformers import AutoTokenizer, AutoModel\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport time\nimport numpy as np","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-08T16:18:38.012956Z","iopub.execute_input":"2024-01-08T16:18:38.013864Z","iopub.status.idle":"2024-01-08T16:18:38.019460Z","shell.execute_reply.started":"2024-01-08T16:18:38.013826Z","shell.execute_reply":"2024-01-08T16:18:38.018315Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Check if GPU is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load the WikiQA dataset\ndataset = load_dataset(\"wiki_qa\")\ntrain_data = dataset['train']\nvalid_data = dataset['validation']\ntest_data = dataset['test']\n\n# Create DataFrames\ntrain_df = pd.DataFrame({'question': train_data['question'], 'document': train_data['document_title'], 'answer': train_data['answer'], 'label': train_data['label']})\nvalid_df = pd.DataFrame({'question': valid_data['question'], 'document': valid_data['document_title'], 'answer': valid_data['answer'], 'label': valid_data['label']})\ntest_df = pd.DataFrame({'question': test_data['question'], 'document': test_data['document_title'], 'answer': test_data['answer'], 'label': test_data['label']})\n\n# Display sample data\nprint(train_df.head())\nprint(valid_df.head())\nprint(test_df.head())","metadata":{"execution":{"iopub.status.busy":"2024-01-08T16:18:38.021322Z","iopub.execute_input":"2024-01-08T16:18:38.021610Z","iopub.status.idle":"2024-01-08T16:18:38.657695Z","shell.execute_reply.started":"2024-01-08T16:18:38.021586Z","shell.execute_reply":"2024-01-08T16:18:38.656777Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70aa3dd80b1f4ac7abcf7e21dcd6990f"}},"metadata":{}},{"name":"stdout","text":"                        question      document  \\\n0  how are glacier caves formed?  Glacier cave   \n1  how are glacier caves formed?  Glacier cave   \n2  how are glacier caves formed?  Glacier cave   \n3  how are glacier caves formed?  Glacier cave   \n4  how are glacier caves formed?  Glacier cave   \n\n                                              answer  label  \n0  A partly submerged glacier cave on Perito More...      0  \n1          The ice facade is approximately 60 m high      0  \n2          Ice formations in the Titlis glacier cave      0  \n3  A glacier cave is a cave formed within the ice...      1  \n4  Glacier caves are often called ice caves , but...      0  \n                                      question          document  \\\n0  How are epithelial tissues joined together?  Tissue (biology)   \n1  How are epithelial tissues joined together?  Tissue (biology)   \n2  How are epithelial tissues joined together?  Tissue (biology)   \n3  How are epithelial tissues joined together?  Tissue (biology)   \n4  How are epithelial tissues joined together?  Tissue (biology)   \n\n                                              answer  label  \n0  Cross section of sclerenchyma fibers in plant ...      0  \n1  Microscopic view of a histologic specimen of h...      0  \n2  In Biology , Tissue is a cellular organization...      0  \n3  A tissue is an ensemble of similar cells from ...      0  \n4  Organs are then formed by the functional group...      0  \n                                          question  \\\n0  HOW AFRICAN AMERICANS WERE IMMIGRATED TO THE US   \n1  HOW AFRICAN AMERICANS WERE IMMIGRATED TO THE US   \n2  HOW AFRICAN AMERICANS WERE IMMIGRATED TO THE US   \n3  HOW AFRICAN AMERICANS WERE IMMIGRATED TO THE US   \n4  HOW AFRICAN AMERICANS WERE IMMIGRATED TO THE US   \n\n                                   document  \\\n0  African immigration to the United States   \n1  African immigration to the United States   \n2  African immigration to the United States   \n3  African immigration to the United States   \n4  African immigration to the United States   \n\n                                              answer  label  \n0  African immigration to the United States refer...      0  \n1  The term African in the scope of this article ...      0  \n2  From the Immigration and Nationality Act of 19...      0  \n3  African immigrants in the United States come f...      0  \n4  They include people from different national, l...      0  \n","output_type":"stream"}]},{"cell_type":"code","source":"# Text preprocessing\nnltk.download('stopwords')\nstop_words = set(stopwords.words('english'))\n\n# Load BERT tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\nmodel = AutoModel.from_pretrained(\"bert-base-uncased\")\nmodel.to(device)\n\n# Function to obtain BERT embeddings for a text\ndef get_bert_embedding(text, model=model, tokenizer=tokenizer, device=device):\n    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n    inputs = {key: value.to(device) for key, value in inputs.items()}\n    with torch.no_grad():\n        outputs = model(**inputs)\n    embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n    return embeddings\n\n# Apply BERT embedding to the DataFrames\ntrain_df['question_embedding'] = train_df['question'].apply(lambda x: get_bert_embedding(x, model, tokenizer, device))\ntrain_df['answer_embedding'] = train_df['answer'].apply(lambda x: get_bert_embedding(x, model, tokenizer, device))\n\nvalid_df['question_embedding'] = valid_df['question'].apply(lambda x: get_bert_embedding(x, model, tokenizer, device))\nvalid_df['answer_embedding'] = valid_df['answer'].apply(lambda x: get_bert_embedding(x, model, tokenizer, device))\n\ntest_df['question_embedding'] = test_df['question'].apply(lambda x: get_bert_embedding(x, model, tokenizer, device))\ntest_df['answer_embedding'] = test_df['answer'].apply(lambda x: get_bert_embedding(x, model, tokenizer, device))\n\n# Display sample preprocessed data with BERT embeddings\nprint(train_df.head())\nprint(valid_df.head())\nprint(test_df.head())","metadata":{"execution":{"iopub.status.busy":"2024-01-08T16:18:38.659859Z","iopub.execute_input":"2024-01-08T16:18:38.660276Z","iopub.status.idle":"2024-01-08T16:30:33.805106Z","shell.execute_reply.started":"2024-01-08T16:18:38.660241Z","shell.execute_reply":"2024-01-08T16:30:33.804069Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n                        question      document  \\\n0  how are glacier caves formed?  Glacier cave   \n1  how are glacier caves formed?  Glacier cave   \n2  how are glacier caves formed?  Glacier cave   \n3  how are glacier caves formed?  Glacier cave   \n4  how are glacier caves formed?  Glacier cave   \n\n                                              answer  label  \\\n0  A partly submerged glacier cave on Perito More...      0   \n1          The ice facade is approximately 60 m high      0   \n2          Ice formations in the Titlis glacier cave      0   \n3  A glacier cave is a cave formed within the ice...      1   \n4  Glacier caves are often called ice caves , but...      0   \n\n                                  question_embedding  \\\n0  [0.32571954, 0.11772122, 0.007693596, 0.038442...   \n1  [0.32571954, 0.11772122, 0.007693596, 0.038442...   \n2  [0.32571954, 0.11772122, 0.007693596, 0.038442...   \n3  [0.32571954, 0.11772122, 0.007693596, 0.038442...   \n4  [0.32571954, 0.11772122, 0.007693596, 0.038442...   \n\n                                    answer_embedding  \n0  [-0.04717919, -0.03647914, -0.06558927, 0.0121...  \n1  [-0.25247642, -0.061092515, 0.17655276, -0.195...  \n2  [0.037123334, 0.06109212, -0.03168871, -0.1536...  \n3  [-0.21420655, 0.23997712, 0.060513545, 0.16253...  \n4  [-0.14883536, 0.43816465, 0.24628332, -0.20578...  \n                                      question          document  \\\n0  How are epithelial tissues joined together?  Tissue (biology)   \n1  How are epithelial tissues joined together?  Tissue (biology)   \n2  How are epithelial tissues joined together?  Tissue (biology)   \n3  How are epithelial tissues joined together?  Tissue (biology)   \n4  How are epithelial tissues joined together?  Tissue (biology)   \n\n                                              answer  label  \\\n0  Cross section of sclerenchyma fibers in plant ...      0   \n1  Microscopic view of a histologic specimen of h...      0   \n2  In Biology , Tissue is a cellular organization...      0   \n3  A tissue is an ensemble of similar cells from ...      0   \n4  Organs are then formed by the functional group...      0   \n\n                                  question_embedding  \\\n0  [0.61421686, 0.044794116, 0.12983055, 0.086350...   \n1  [0.61421686, 0.044794116, 0.12983055, 0.086350...   \n2  [0.61421686, 0.044794116, 0.12983055, 0.086350...   \n3  [0.61421686, 0.044794116, 0.12983055, 0.086350...   \n4  [0.61421686, 0.044794116, 0.12983055, 0.086350...   \n\n                                    answer_embedding  \n0  [0.024858873, -0.28060278, -0.1142582, 0.10347...  \n1  [-0.123608224, -0.11768918, -0.19715111, -0.42...  \n2  [0.19751295, -0.011683886, -0.34795427, -0.040...  \n3  [0.25850248, -0.09799119, 0.07040265, 0.054102...  \n4  [0.2959702, 0.2425332, 0.48161456, 0.14423575,...  \n                                          question  \\\n0  HOW AFRICAN AMERICANS WERE IMMIGRATED TO THE US   \n1  HOW AFRICAN AMERICANS WERE IMMIGRATED TO THE US   \n2  HOW AFRICAN AMERICANS WERE IMMIGRATED TO THE US   \n3  HOW AFRICAN AMERICANS WERE IMMIGRATED TO THE US   \n4  HOW AFRICAN AMERICANS WERE IMMIGRATED TO THE US   \n\n                                   document  \\\n0  African immigration to the United States   \n1  African immigration to the United States   \n2  African immigration to the United States   \n3  African immigration to the United States   \n4  African immigration to the United States   \n\n                                              answer  label  \\\n0  African immigration to the United States refer...      0   \n1  The term African in the scope of this article ...      0   \n2  From the Immigration and Nationality Act of 19...      0   \n3  African immigrants in the United States come f...      0   \n4  They include people from different national, l...      0   \n\n                                  question_embedding  \\\n0  [0.23752125, -0.097810455, -0.44925365, -0.130...   \n1  [0.23752125, -0.097810455, -0.44925365, -0.130...   \n2  [0.23752125, -0.097810455, -0.44925365, -0.130...   \n3  [0.23752125, -0.097810455, -0.44925365, -0.130...   \n4  [0.23752125, -0.097810455, -0.44925365, -0.130...   \n\n                                    answer_embedding  \n0  [0.05626864, 0.27764466, -0.14057814, -0.08460...  \n1  [0.03041647, 0.11533032, -0.41976067, -0.32445...  \n2  [-0.33863014, 0.06937396, -0.025464684, 0.0669...  \n3  [-0.12071531, 0.040707868, 0.0018560679, -0.20...  \n4  [0.3674008, 0.53715247, 0.31823918, -0.2417825...  \n","output_type":"stream"}]},{"cell_type":"code","source":"# Function to perform semantic search on a query using cosine similarity with BERT embeddings\ndef semantic_search_bert(query_embedding, document_embeddings):\n    similarities = cosine_similarity([query_embedding], document_embeddings)[0]\n    return similarities\n\n# Perform semantic search for 100 queries using BERT embeddings\nnum_queries = 100\nmap_scores = []\nelapsed_times = []\n\nfor i in range(num_queries):\n    # Randomly select a query from the test set\n    query_row = test_df.sample(1).iloc[0]\n    query_embedding = get_bert_embedding(query_row['question'])\n    \n    # Perform semantic search and measure time elapsed\n    start_time = time.time()\n    predictions = semantic_search_bert(query_embedding, np.vstack(test_df['answer_embedding'].values))\n    elapsed_time = time.time() - start_time\n    \n    # Calculate MAP for the query\n    true_labels = test_df['label'].values\n    map_score = average_precision_score(true_labels, predictions)\n    \n    # Append results to lists\n    map_scores.append(map_score)\n    elapsed_times.append(elapsed_time)\n\n# Calculate average MAP and average time elapsed\naverage_map = np.mean(map_scores)\naverage_time_elapsed = np.mean(elapsed_times)\n\n# Display results\nprint(f\"Average MAP for {num_queries} queries: {average_map}\")\nprint(f\"Average time elapsed for each query: {average_time_elapsed} seconds\")","metadata":{"execution":{"iopub.status.busy":"2024-01-08T16:30:33.806249Z","iopub.execute_input":"2024-01-08T16:30:33.806559Z","iopub.status.idle":"2024-01-08T16:30:42.111302Z","shell.execute_reply.started":"2024-01-08T16:30:33.806532Z","shell.execute_reply":"2024-01-08T16:30:42.110329Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Average MAP for 100 queries: 0.04124547094451735\nAverage time elapsed for each query: 0.05129656076431274 seconds\n","output_type":"stream"}]},{"cell_type":"code","source":"def search(query, df):\n    # Get BERT embedding for the query\n    query_embedding = get_bert_embedding(query)\n    \n    # Calculate cosine similarity with answer embeddings\n    similarities = semantic_search_bert(query_embedding, np.vstack(df['answer_embedding'].values))\n    \n    # Get indices of the top 3 hits\n    top_indices = np.argsort(similarities)[::-1][:3]\n    \n    # Display the top 3 hits and their similarity scores\n    print(f\"Query: {query}\\n\")\n    print(\"Top 3 Hits:\")\n    for i, idx in enumerate(top_indices, 1):\n        print(f\"{i}. Answer: {df.iloc[idx]['answer']}\")\n        print(f\"   Similarity Score: {similarities[idx]:.4f}\\n\")\n\n# Example usage:\nquery_example = \"What is the capital of the United States?\"\nsearch(query_example, test_df)\n\n","metadata":{"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Query: What is the capital of the United States?\n\nTop 3 Hits:\n1. Answer: England () is a country that is part of the United Kingdom .\n   Similarity Score: 0.6822\n\n2. Answer: Map of the world of Muslim majority areas.\n   Similarity Score: 0.6738\n\n3. Answer: The president is frequently described as the most powerful person in the world.\n   Similarity Score: 0.6718\n\n","output_type":"stream"}]}]}