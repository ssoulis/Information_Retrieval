{"cells":[{"cell_type":"code","execution_count":12,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-01-08T16:37:30.329426Z","iopub.status.busy":"2024-01-08T16:37:30.329040Z","iopub.status.idle":"2024-01-08T16:37:30.335464Z","shell.execute_reply":"2024-01-08T16:37:30.334380Z","shell.execute_reply.started":"2024-01-08T16:37:30.329396Z"},"trusted":true},"outputs":[],"source":["from datasets import load_dataset\n","import pandas as pd\n","import nltk\n","import torch\n","from nltk.corpus import stopwords\n","from sklearn.metrics import average_precision_score\n","from transformers import AutoTokenizer, AutoModel\n","from sklearn.metrics.pairwise import cosine_similarity\n","import time\n","import numpy as np"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-01-08T16:37:30.340610Z","iopub.status.busy":"2024-01-08T16:37:30.340245Z","iopub.status.idle":"2024-01-08T16:37:31.086828Z","shell.execute_reply":"2024-01-08T16:37:31.085698Z","shell.execute_reply.started":"2024-01-08T16:37:30.340581Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c52748fb2dd74c3f846cc63fb70a287d","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["                        question      document  \\\n","0  how are glacier caves formed?  Glacier cave   \n","1  how are glacier caves formed?  Glacier cave   \n","2  how are glacier caves formed?  Glacier cave   \n","3  how are glacier caves formed?  Glacier cave   \n","4  how are glacier caves formed?  Glacier cave   \n","\n","                                              answer  label  \n","0  A partly submerged glacier cave on Perito More...      0  \n","1          The ice facade is approximately 60 m high      0  \n","2          Ice formations in the Titlis glacier cave      0  \n","3  A glacier cave is a cave formed within the ice...      1  \n","4  Glacier caves are often called ice caves , but...      0  \n","                                      question          document  \\\n","0  How are epithelial tissues joined together?  Tissue (biology)   \n","1  How are epithelial tissues joined together?  Tissue (biology)   \n","2  How are epithelial tissues joined together?  Tissue (biology)   \n","3  How are epithelial tissues joined together?  Tissue (biology)   \n","4  How are epithelial tissues joined together?  Tissue (biology)   \n","\n","                                              answer  label  \n","0  Cross section of sclerenchyma fibers in plant ...      0  \n","1  Microscopic view of a histologic specimen of h...      0  \n","2  In Biology , Tissue is a cellular organization...      0  \n","3  A tissue is an ensemble of similar cells from ...      0  \n","4  Organs are then formed by the functional group...      0  \n","                                          question  \\\n","0  HOW AFRICAN AMERICANS WERE IMMIGRATED TO THE US   \n","1  HOW AFRICAN AMERICANS WERE IMMIGRATED TO THE US   \n","2  HOW AFRICAN AMERICANS WERE IMMIGRATED TO THE US   \n","3  HOW AFRICAN AMERICANS WERE IMMIGRATED TO THE US   \n","4  HOW AFRICAN AMERICANS WERE IMMIGRATED TO THE US   \n","\n","                                   document  \\\n","0  African immigration to the United States   \n","1  African immigration to the United States   \n","2  African immigration to the United States   \n","3  African immigration to the United States   \n","4  African immigration to the United States   \n","\n","                                              answer  label  \n","0  African immigration to the United States refer...      0  \n","1  The term African in the scope of this article ...      0  \n","2  From the Immigration and Nationality Act of 19...      0  \n","3  African immigrants in the United States come f...      0  \n","4  They include people from different national, l...      0  \n"]}],"source":["# Check if GPU is available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Load the WikiQA dataset\n","dataset = load_dataset(\"wiki_qa\")\n","train_data = dataset['train']\n","valid_data = dataset['validation']\n","test_data = dataset['test']\n","\n","# Create DataFrames\n","train_df = pd.DataFrame({'question': train_data['question'], 'document': train_data['document_title'], 'answer': train_data['answer'], 'label': train_data['label']})\n","valid_df = pd.DataFrame({'question': valid_data['question'], 'document': valid_data['document_title'], 'answer': valid_data['answer'], 'label': valid_data['label']})\n","test_df = pd.DataFrame({'question': test_data['question'], 'document': test_data['document_title'], 'answer': test_data['answer'], 'label': test_data['label']})\n","\n","# Display sample data\n","print(train_df.head())\n","print(valid_df.head())\n","print(test_df.head())"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-01-08T16:37:31.090083Z","iopub.status.busy":"2024-01-08T16:37:31.089646Z","iopub.status.idle":"2024-01-08T16:37:52.699041Z","shell.execute_reply":"2024-01-08T16:37:52.698078Z","shell.execute_reply.started":"2024-01-08T16:37:31.090043Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","               question      document  \\\n","0  glacier caves formed  Glacier cave   \n","1  glacier caves formed  Glacier cave   \n","2  glacier caves formed  Glacier cave   \n","3  glacier caves formed  Glacier cave   \n","4  glacier caves formed  Glacier cave   \n","\n","                                              answer  label  \n","0  partly submerged glacier cave perito moreno gl...      0  \n","1                      ice facade approximately high      0  \n","2                 ice formations titlis glacier cave      0  \n","3        glacier cave cave formed within ice glacier      1  \n","4  glacier caves often called ice caves term prop...      0  \n","                             question          document  \\\n","0  epithelial tissues joined together  Tissue (biology)   \n","1  epithelial tissues joined together  Tissue (biology)   \n","2  epithelial tissues joined together  Tissue (biology)   \n","3  epithelial tissues joined together  Tissue (biology)   \n","4  epithelial tissues joined together  Tissue (biology)   \n","\n","                                              answer  label  \n","0  cross section sclerenchyma fibers plant ground...      0  \n","1  microscopic view histologic specimen human lun...      0  \n","2  biology tissue cellular organizational level i...      0  \n","3  tissue ensemble similar cells origin together ...      0  \n","4  organs formed functional grouping together mul...      0  \n","                          question                                  document  \\\n","0  african americans immigrated us  African immigration to the United States   \n","1  african americans immigrated us  African immigration to the United States   \n","2  african americans immigrated us  African immigration to the United States   \n","3  african americans immigrated us  African immigration to the United States   \n","4  african americans immigrated us  African immigration to the United States   \n","\n","                                              answer  label  \n","0  african immigration united states refers immig...      0  \n","1  term african scope article refers geographical...      0  \n","2  immigration nationality act estimated total mi...      0  \n","3  african immigrants united states come almost r...      0  \n","4  include people different national linguistic e...      0  \n"]}],"source":["import re\n","from nltk.tokenize import word_tokenize\n","\n","# Text preprocessing\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","\n","def preprocess_text(text):\n","    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n","    text = text.lower()\n","    tokens = word_tokenize(text)\n","    stop_words = set(stopwords.words('english'))\n","    tokens = [token for token in tokens if token not in stop_words]\n","    preprocessed_text = ' '.join(tokens)\n","    return preprocessed_text\n","\n","# Apply text preprocessing to the DataFrames\n","train_df['question'] = train_df['question'].apply(preprocess_text)\n","train_df['answer'] = train_df['answer'].apply(preprocess_text)\n","\n","valid_df['question'] = valid_df['question'].apply(preprocess_text)\n","valid_df['answer'] = valid_df['answer'].apply(preprocess_text)\n","\n","test_df['question'] = test_df['question'].apply(preprocess_text)\n","test_df['answer'] = test_df['answer'].apply(preprocess_text)\n","\n","# Display sample preprocessed data\n","print(train_df.head())\n","print(valid_df.head())\n","print(test_df.head())"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-01-08T16:37:52.700737Z","iopub.status.busy":"2024-01-08T16:37:52.700371Z","iopub.status.idle":"2024-01-08T16:38:22.672138Z","shell.execute_reply":"2024-01-08T16:38:22.670690Z","shell.execute_reply.started":"2024-01-08T16:37:52.700702Z"},"trusted":true},"outputs":[],"source":["from transformers import  BertTokenizer, AdamW\n","\n","# Tokenize and encode the text data\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","def tokenize_data(data, max_length=128):\n","    encodings = tokenizer(data['question'].tolist(), data['answer'].tolist(), truncation=True, padding='max_length', max_length=max_length, return_tensors='pt')\n","    return encodings\n","\n","# Tokenize the data\n","train_tokenized = tokenize_data(train_df)\n","valid_tokenized = tokenize_data(valid_df)\n","test_tokenized = tokenize_data(test_df)\n","\n","# Convert labels to tensors\n","train_labels = torch.tensor(train_df['label'].values)\n","valid_labels = torch.tensor(valid_df['label'].values)\n","test_labels = torch.tensor(test_df['label'].values)"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-01-08T16:38:22.674395Z","iopub.status.busy":"2024-01-08T16:38:22.673755Z","iopub.status.idle":"2024-01-08T17:01:52.014640Z","shell.execute_reply":"2024-01-08T17:01:52.013732Z","shell.execute_reply.started":"2024-01-08T16:38:22.674357Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","/tmp/ipykernel_42/424451333.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","/tmp/ipykernel_42/424451333.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  item['label'] = torch.tensor(self.labels[idx])\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/3, Loss: 0.23966432761918746\n","Epoch 2/3, Loss: 0.1612970909014324\n","Epoch 3/3, Loss: 0.10100924557298815\n"]}],"source":["from transformers import  BertModel\n","import torch\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","\n","\n","# Define the dataset\n","class QAClassificationDataset(Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        item['label'] = torch.tensor(self.labels[idx])\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","# Create datasets and dataloaders\n","train_dataset = QAClassificationDataset(train_tokenized, train_labels)\n","valid_dataset = QAClassificationDataset(valid_tokenized, valid_labels)\n","test_dataset = QAClassificationDataset(test_tokenized, test_labels)\n","\n","train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n","valid_loader = DataLoader(valid_dataset, batch_size=8, shuffle=False)\n","test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n","\n","# Model training\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Fine-tuned BERT for sequence classification\n","model_classifier = BertModel.from_pretrained(\"bert-base-uncased\", num_labels=2).to(device)\n","optimizer = AdamW(model_classifier.parameters(), lr=5e-5)\n","\n","# Training function\n","def train_classification_model(model, train_loader, optimizer, num_epochs=3):\n","    criterion = torch.nn.CrossEntropyLoss()\n","\n","    for epoch in range(num_epochs):\n","        model.train()\n","        total_loss = 0.0\n","\n","        for batch in train_loader:\n","            inputs = {key: value.to(device) for key, value in batch.items() if key != 'label'}\n","            labels = batch['label'].to(device)\n","\n","            outputs = model(**inputs)\n","            logits = outputs.last_hidden_state[:, 0, :]  # Extract the [CLS] token representation\n","            loss = criterion(logits, labels)\n","\n","            total_loss += loss.item()\n","\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","        average_loss = total_loss / len(train_loader)\n","        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {average_loss}\")\n","\n","\n","# Train the classification model\n","train_classification_model(model_classifier, train_loader, optimizer)"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-01-08T17:14:41.456316Z","iopub.status.busy":"2024-01-08T17:14:41.455328Z","iopub.status.idle":"2024-01-08T17:14:42.785038Z","shell.execute_reply":"2024-01-08T17:14:42.783969Z","shell.execute_reply.started":"2024-01-08T17:14:41.456248Z"},"trusted":true},"outputs":[],"source":["# Save the trained model\n","model_save_path = \"/kaggle/working/\"\n","model_classifier.save_pretrained(model_save_path)"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-01-08T17:01:52.938169Z","iopub.status.busy":"2024-01-08T17:01:52.937890Z","iopub.status.idle":"2024-01-08T17:02:38.707851Z","shell.execute_reply":"2024-01-08T17:02:38.706574Z","shell.execute_reply.started":"2024-01-08T17:01:52.938146Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_42/424451333.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","/tmp/ipykernel_42/424451333.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  item['label'] = torch.tensor(self.labels[idx])\n"]},{"name":"stdout","output_type":"stream","text":["Average Precision: 0.10577967167272476\n","Accuracy: 0.9347931873479318\n","Precision: 0.2845849802371542\n","Recall: 0.24573378839590443\n","F1-score: 0.26373626373626374\n","\n","Confusion Matrix:\n","[[5691  181]\n"," [ 221   72]]\n","\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.96      0.97      0.97      5872\n","           1       0.28      0.25      0.26       293\n","\n","    accuracy                           0.93      6165\n","   macro avg       0.62      0.61      0.61      6165\n","weighted avg       0.93      0.93      0.93      6165\n","\n"]}],"source":["import torch.nn.functional as F\n","import numpy as np\n","from sklearn.metrics import average_precision_score\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","def evaluate_classification_model(model, data_loader):\n","    model.eval()\n","    all_labels = []\n","    all_predictions = []\n","\n","    with torch.no_grad():\n","        for batch in data_loader:\n","            inputs = {key: value.to(device) for key, value in batch.items() if key != 'label'}\n","            labels = batch['label'].to(device)\n","\n","            outputs = model(**inputs)\n","            logits = outputs.last_hidden_state[:, 0, :]  # Extract the [CLS] token representation\n","            predictions = torch.argmax(logits, dim=1)\n","\n","            all_labels.extend(labels.cpu().numpy())\n","            all_predictions.extend(predictions.cpu().numpy())\n","\n","    return all_labels, all_predictions\n","\n","# Evaluate the model on the test data\n","test_labels, test_predictions = evaluate_classification_model(model_classifier, test_loader)\n","\n","# Calculate evaluation metrics\n","average_precision = average_precision_score(test_labels, test_predictions)\n","print(f\"Average Precision: {average_precision}\")\n","\n","\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n","\n","# Assuming test_labels and test_predictions represent binary relevance labels (1 for relevant, 0 for not relevant)\n","\n","# Calculate additional classification metrics\n","accuracy = accuracy_score(test_labels, test_predictions)\n","precision = precision_score(test_labels, test_predictions)\n","recall = recall_score(test_labels, test_predictions)\n","f1 = f1_score(test_labels, test_predictions)\n","\n","# Print the metrics\n","print(f\"Accuracy: {accuracy}\")\n","print(f\"Precision: {precision}\")\n","print(f\"Recall: {recall}\")\n","print(f\"F1-score: {f1}\")\n","\n","# Confusion Matrix\n","conf_matrix = confusion_matrix(test_labels, test_predictions)\n","print(\"\\nConfusion Matrix:\")\n","print(conf_matrix)\n","\n","# Classification Report\n","class_report = classification_report(test_labels, test_predictions)\n","print(\"\\nClassification Report:\")\n","print(class_report)\n","\n"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-01-08T17:02:38.710221Z","iopub.status.busy":"2024-01-08T17:02:38.709795Z","iopub.status.idle":"2024-01-08T17:02:39.280327Z","shell.execute_reply":"2024-01-08T17:02:39.279267Z","shell.execute_reply.started":"2024-01-08T17:02:38.710182Z"},"trusted":true},"outputs":[{"data":{"text/plain":["BertModel(\n","  (embeddings): BertEmbeddings(\n","    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","    (position_embeddings): Embedding(512, 768)\n","    (token_type_embeddings): Embedding(2, 768)\n","    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (encoder): BertEncoder(\n","    (layer): ModuleList(\n","      (0-11): 12 x BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","  )\n","  (pooler): BertPooler(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (activation): Tanh()\n","  )\n",")"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["from transformers import BertModel, BertTokenizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","import time\n","import numpy as np\n","\n","# Replace \"your_model_path\" with the actual path where your fine-tuned model is saved\n","fine_tuned_model_path = \"/kaggle/working/\"\n","\n","# Load the fine-tuned BERT model and tokenizer\n","fine_tuned_model = BertModel.from_pretrained(fine_tuned_model_path)\n","# Load the standard BERT tokenizer\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n","\n","# Move the model to the device\n","fine_tuned_model.to(device)"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-01-08T17:02:39.281903Z","iopub.status.busy":"2024-01-08T17:02:39.281595Z","iopub.status.idle":"2024-01-08T17:14:33.011639Z","shell.execute_reply":"2024-01-08T17:14:33.010606Z","shell.execute_reply.started":"2024-01-08T17:02:39.281877Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["               question      document  \\\n","0  glacier caves formed  Glacier cave   \n","1  glacier caves formed  Glacier cave   \n","2  glacier caves formed  Glacier cave   \n","3  glacier caves formed  Glacier cave   \n","4  glacier caves formed  Glacier cave   \n","\n","                                              answer  label  \\\n","0  partly submerged glacier cave perito moreno gl...      0   \n","1                      ice facade approximately high      0   \n","2                 ice formations titlis glacier cave      0   \n","3        glacier cave cave formed within ice glacier      1   \n","4  glacier caves often called ice caves term prop...      0   \n","\n","                                  question_embedding  \\\n","0  [10.352801, 7.9546432, -0.21673298, -0.1958606...   \n","1  [10.352801, 7.9546432, -0.21673298, -0.1958606...   \n","2  [10.352801, 7.9546432, -0.21673298, -0.1958606...   \n","3  [10.352801, 7.9546432, -0.21673298, -0.1958606...   \n","4  [10.352801, 7.9546432, -0.21673298, -0.1958606...   \n","\n","                                  document_embedding  \n","0  [10.591792, 8.387752, -0.20782095, -0.22744042...  \n","1  [10.591792, 8.387752, -0.20782095, -0.22744042...  \n","2  [10.591792, 8.387752, -0.20782095, -0.22744042...  \n","3  [10.591792, 8.387752, -0.20782095, -0.22744042...  \n","4  [10.591792, 8.387752, -0.20782095, -0.22744042...  \n","                             question          document  \\\n","0  epithelial tissues joined together  Tissue (biology)   \n","1  epithelial tissues joined together  Tissue (biology)   \n","2  epithelial tissues joined together  Tissue (biology)   \n","3  epithelial tissues joined together  Tissue (biology)   \n","4  epithelial tissues joined together  Tissue (biology)   \n","\n","                                              answer  label  \\\n","0  cross section sclerenchyma fibers plant ground...      0   \n","1  microscopic view histologic specimen human lun...      0   \n","2  biology tissue cellular organizational level i...      0   \n","3  tissue ensemble similar cells origin together ...      0   \n","4  organs formed functional grouping together mul...      0   \n","\n","                                  question_embedding  \\\n","0  [10.474598, 8.140356, -0.23868819, -0.2862664,...   \n","1  [10.474598, 8.140356, -0.23868819, -0.2862664,...   \n","2  [10.474598, 8.140356, -0.23868819, -0.2862664,...   \n","3  [10.474598, 8.140356, -0.23868819, -0.2862664,...   \n","4  [10.474598, 8.140356, -0.23868819, -0.2862664,...   \n","\n","                                  document_embedding  \n","0  [10.181678, 8.060405, -0.269181, -0.20784315, ...  \n","1  [10.181678, 8.060405, -0.269181, -0.20784315, ...  \n","2  [10.181678, 8.060405, -0.269181, -0.20784315, ...  \n","3  [10.181678, 8.060405, -0.269181, -0.20784315, ...  \n","4  [10.181678, 8.060405, -0.269181, -0.20784315, ...  \n","                          question                                  document  \\\n","0  african americans immigrated us  African immigration to the United States   \n","1  african americans immigrated us  African immigration to the United States   \n","2  african americans immigrated us  African immigration to the United States   \n","3  african americans immigrated us  African immigration to the United States   \n","4  african americans immigrated us  African immigration to the United States   \n","\n","                                              answer  label  \\\n","0  african immigration united states refers immig...      0   \n","1  term african scope article refers geographical...      0   \n","2  immigration nationality act estimated total mi...      0   \n","3  african immigrants united states come almost r...      0   \n","4  include people different national linguistic e...      0   \n","\n","                                  question_embedding  \\\n","0  [10.032108, 8.212364, -0.21114549, -0.15805662...   \n","1  [10.032108, 8.212364, -0.21114549, -0.15805662...   \n","2  [10.032108, 8.212364, -0.21114549, -0.15805662...   \n","3  [10.032108, 8.212364, -0.21114549, -0.15805662...   \n","4  [10.032108, 8.212364, -0.21114549, -0.15805662...   \n","\n","                                  document_embedding  \n","0  [9.919575, 8.00239, -0.25112045, -0.08448837, ...  \n","1  [9.919575, 8.00239, -0.25112045, -0.08448837, ...  \n","2  [9.919575, 8.00239, -0.25112045, -0.08448837, ...  \n","3  [9.919575, 8.00239, -0.25112045, -0.08448837, ...  \n","4  [9.919575, 8.00239, -0.25112045, -0.08448837, ...  \n"]}],"source":["import torch\n","\n","def get_bert_embedding(text, model, tokenizer, device):\n","    # Tokenize input text\n","    tokens = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n","    \n","    # Move the input to the device\n","    for key in tokens:\n","        tokens[key] = tokens[key].to(device)\n","\n","    # Get the model output\n","    with torch.no_grad():\n","        output = model(**tokens)\n","\n","    # Extract the embeddings from the model output\n","    embeddings = output.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n","\n","    return embeddings\n","\n","\n","# Apply fine-tuned BERT embedding to the DataFrames\n","train_df['question_embedding'] = train_df['question'].apply(lambda x: get_bert_embedding(x, fine_tuned_model, tokenizer, device))\n","train_df['document_embedding'] = train_df['document'].apply(lambda x: get_bert_embedding(x, fine_tuned_model, tokenizer, device))\n","\n","valid_df['question_embedding'] = valid_df['question'].apply(lambda x: get_bert_embedding(x, fine_tuned_model, tokenizer, device))\n","valid_df['document_embedding'] = valid_df['document'].apply(lambda x: get_bert_embedding(x, fine_tuned_model, tokenizer, device))\n","\n","test_df['question_embedding'] = test_df['question'].apply(lambda x: get_bert_embedding(x, fine_tuned_model, tokenizer, device))\n","test_df['document_embedding'] = test_df['document'].apply(lambda x: get_bert_embedding(x, fine_tuned_model, tokenizer, device))\n","\n","# Display sample preprocessed data with fine-tuned BERT embeddings\n","print(train_df.head())\n","print(valid_df.head())\n","print(test_df.head())\n"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-01-08T17:14:33.012939Z","iopub.status.busy":"2024-01-08T17:14:33.012631Z","iopub.status.idle":"2024-01-08T17:14:41.450710Z","shell.execute_reply":"2024-01-08T17:14:41.449392Z","shell.execute_reply.started":"2024-01-08T17:14:33.012905Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Average MAP for 100 queries using fine-tuned BERT: 0.050003662531048355\n","Average time elapsed for each query using fine-tuned BERT: 0.05123048543930054 seconds\n"]}],"source":["# Function to perform semantic search on a query using cosine similarity with fine-tuned BERT embeddings\n","def semantic_search_fine_tuned_bert(query_embedding, document_embeddings):\n","    similarities = cosine_similarity([query_embedding], document_embeddings)[0]\n","    return similarities\n","\n","# Perform semantic search for 100 queries using fine-tuned BERT embeddings\n","num_queries = 100\n","map_scores_fine_tuned = []\n","elapsed_times_fine_tuned = []\n","\n","for i in range(num_queries):\n","    # Randomly select a query from the test set\n","    query_row = test_df.sample(1).iloc[0]\n","    query_embedding = get_bert_embedding(query_row['question'], fine_tuned_model, tokenizer, device)\n","    \n","    # Perform semantic search and measure time elapsed\n","    start_time = time.time()\n","    predictions = semantic_search_fine_tuned_bert(query_embedding, np.vstack(test_df['document_embedding'].values))\n","    elapsed_time = time.time() - start_time\n","    \n","    # Calculate MAP for the query\n","    true_labels = test_df['label'].values\n","    map_score = average_precision_score(true_labels, predictions)\n","    \n","    # Append results to lists\n","    map_scores_fine_tuned.append(map_score)\n","    elapsed_times_fine_tuned.append(elapsed_time)\n","\n","# Calculate average MAP and average time elapsed for fine-tuned BERT\n","average_map_fine_tuned = np.mean(map_scores_fine_tuned)\n","average_time_elapsed_fine_tuned = np.mean(elapsed_times_fine_tuned)\n","\n","# Display results for fine-tuned BERT\n","print(f\"Average MAP for {num_queries} queries using fine-tuned BERT: {average_map_fine_tuned}\")\n","print(f\"Average time elapsed for each query using fine-tuned BERT: {average_time_elapsed_fine_tuned} seconds\")"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2024-01-08T17:34:49.523194Z","iopub.status.busy":"2024-01-08T17:34:49.522455Z","iopub.status.idle":"2024-01-08T17:47:24.931308Z","shell.execute_reply":"2024-01-08T17:47:24.927764Z","shell.execute_reply.started":"2024-01-08T17:34:49.523160Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a2ac948459de40b49a0724193d230981","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["                        question      document  \\\n","0  how are glacier caves formed?  Glacier cave   \n","1  how are glacier caves formed?  Glacier cave   \n","2  how are glacier caves formed?  Glacier cave   \n","3  how are glacier caves formed?  Glacier cave   \n","4  how are glacier caves formed?  Glacier cave   \n","\n","                                              answer  label  \\\n","0  A partly submerged glacier cave on Perito More...      0   \n","1          The ice facade is approximately 60 m high      0   \n","2          Ice formations in the Titlis glacier cave      0   \n","3  A glacier cave is a cave formed within the ice...      1   \n","4  Glacier caves are often called ice caves , but...      0   \n","\n","                                  question_embedding  \\\n","0  [0.32571954, 0.11772122, 0.007693596, 0.038442...   \n","1  [0.32571954, 0.11772122, 0.007693596, 0.038442...   \n","2  [0.32571954, 0.11772122, 0.007693596, 0.038442...   \n","3  [0.32571954, 0.11772122, 0.007693596, 0.038442...   \n","4  [0.32571954, 0.11772122, 0.007693596, 0.038442...   \n","\n","                                    answer_embedding  \n","0  [-0.04717919, -0.03647914, -0.06558927, 0.0121...  \n","1  [-0.25247642, -0.061092515, 0.17655276, -0.195...  \n","2  [0.037123334, 0.06109212, -0.03168871, -0.1536...  \n","3  [-0.21420655, 0.23997712, 0.060513545, 0.16253...  \n","4  [-0.14883536, 0.43816465, 0.24628332, -0.20578...  \n","Query: What is the capital of the United States?\n","\n","Top 3 Hits:\n","1. Answer: Eight other cities have served as the meeting place for Congress and are therefore considered to have once been the capital of the United States.\n","   Similarity Score: 0.7896\n","\n","2. Answer: In addition, each of the 50 U.S. states and the five principal territories of the United States maintains its own capital.\n","   Similarity Score: 0.7800\n","\n","3. Answer: Washington, D.C. has been the capital of the United States since 1800.\n","   Similarity Score: 0.7795\n","\n"]}],"source":["# Check if GPU is available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Load the WikiQA dataset\n","dataset = load_dataset(\"wiki_qa\")\n","train_df = pd.DataFrame({'question': dataset['train']['question'], 'document': dataset['train']['document_title'], 'answer': dataset['train']['answer'], 'label': dataset['train']['label']})\n","valid_df = pd.DataFrame({'question': dataset['validation']['question'], 'document': dataset['validation']['document_title'], 'answer': dataset['validation']['answer'], 'label': dataset['validation']['label']})\n","test_df = pd.DataFrame({'question': dataset['test']['question'], 'document': dataset['test']['document_title'], 'answer': dataset['test']['answer'], 'label': dataset['test']['label']})\n","\n","# Apply BERT embedding to the DataFrames\n","train_df['question_embedding'] = train_df['question'].apply(lambda x: get_bert_embedding(x, model, tokenizer, device))\n","train_df['answer_embedding'] = train_df['answer'].apply(lambda x: get_bert_embedding(x, model, tokenizer, device))\n","\n","valid_df['question_embedding'] = valid_df['question'].apply(lambda x: get_bert_embedding(x, model, tokenizer, device))\n","valid_df['answer_embedding'] = valid_df['answer'].apply(lambda x: get_bert_embedding(x, model, tokenizer, device))\n","\n","test_df['question_embedding'] = test_df['question'].apply(lambda x: get_bert_embedding(x, model, tokenizer, device))\n","test_df['answer_embedding'] = test_df['answer'].apply(lambda x: get_bert_embedding(x, model, tokenizer, device))\n","\n","# Concatenate the DataFrames\n","df = pd.concat([train_df, valid_df, test_df], ignore_index=True)\n","\n","# Display sample data\n","print(df.head())\n","\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","def search(query, df, model, tokenizer, all_questions_embeddings, all_answers_embeddings):\n","    # Get BERT embedding for the query\n","    query_embedding = get_bert_embedding(query, model, tokenizer, device)\n","    \n","    # Calculate cosine similarity with all question embeddings\n","    question_similarities = cosine_similarity([query_embedding], all_questions_embeddings)[0]\n","    \n","    # Calculate cosine similarity with all answer embeddings\n","    answer_similarities = cosine_similarity([query_embedding], all_answers_embeddings)[0]\n","    \n","    # Combine similarities from both question and answer embeddings\n","    combined_similarities = (question_similarities + answer_similarities) / 2\n","    \n","    # Get indices of the top 3 hits\n","    top_indices = np.argsort(combined_similarities)[::-1][:3]\n","    \n","    # Display the top 3 hits and their similarity scores\n","    print(f\"Query: {query}\\n\")\n","    print(\"Top 3 Hits:\")\n","    for i, idx in enumerate(top_indices, 1):\n","        print(f\"{i}. Answer: {df.iloc[idx]['answer']}\")\n","        print(f\"   Similarity Score: {combined_similarities[idx]:.4f}\\n\")\n","\n","# Extract all question and answer embeddings from the entire dataset\n","all_questions_embeddings = np.vstack(df['question_embedding'].values)\n","all_answers_embeddings = np.vstack(df['answer_embedding'].values)\n","\n","# Example usage:\n","query_example = \"What is the capital of the United States?\"\n","search(query_example, df, model, tokenizer, all_questions_embeddings, all_answers_embeddings)\n","\n"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":6763,"sourceId":9801,"sourceType":"datasetVersion"}],"dockerImageVersionId":30627,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
